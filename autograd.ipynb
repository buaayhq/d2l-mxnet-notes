{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- import `autograd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n[[0.]\n [1.]\n [2.]\n [3.]]\n<NDArray 4x1 @cpu(0)>\n"
    }
   ],
   "source": [
    "from mxnet import autograd, nd\n",
    "x = nd.arange(4).reshape((4,1))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacht grad to x\n",
    "- It allocates memory to store its gradient, which has the same shape as x\n",
    "- It also tell the system to compute gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[0.]\n [0.]\n [0.]\n [0.]]\n<NDArray 4x1 @cpu(0)>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.attach_grad()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, we compute\n",
    "$$y = 2\\mathbf{x}^{\\top}\\mathbf{x}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[28.]]\n<NDArray 1x1 @cpu(0)>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2*nd.dot(x.T, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MXNetError",
     "evalue": "[10:22:54] C:\\Jenkins\\workspace\\mxnet-tag\\mxnet\\src\\imperative\\imperative.cc:295: Check failed: !AGInfo::IsNone(*i): Cannot differentiate node because it is not in a computational graph. You need to set is_recording to true or use autograd.record() to save computational graphs for backward. If you want to differentiate the same graph twice, you need to pass retain_graph=True to backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b23f1bbaf10a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\Lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, out_grad, retain_graph, train_mode)\u001b[0m\n\u001b[0;32m   2214\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2215\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2216\u001b[1;33m             ctypes.c_void_p(0)))\n\u001b[0m\u001b[0;32m   2217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtostype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\Lib\\site-packages\\mxnet\\base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \"\"\"\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMXNetError\u001b[0m: [10:22:54] C:\\Jenkins\\workspace\\mxnet-tag\\mxnet\\src\\imperative\\imperative.cc:295: Check failed: !AGInfo::IsNone(*i): Cannot differentiate node because it is not in a computational graph. You need to set is_recording to true or use autograd.record() to save computational graphs for backward. If you want to differentiate the same graph twice, you need to pass retain_graph=True to backward."
     ]
    }
   ],
   "source": [
    "# backward\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**We must place code inside a with autograd.record() block. Then mxnet wiil build the according computation graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[ 0.]\n [ 4.]\n [ 8.]\n [12.]]\n<NDArray 4x1 @cpu(0)>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = 2*nd.dot(x.T, x)\n",
    "# `y.backward()` equals to `y.sum().backward()`\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- verify:\n",
    "for $y=2\\mathbf{x}^\\top\\mathbf{x}$, we get $\\frac{\\partial y}{\\partial\\mathbf{x}}=4\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[0.]\n [0.]\n [0.]\n [0.]]\n<NDArray 4x1 @cpu(0)>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad-4*x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- the `record` scope will alter the mode by assuming that gradient is only required for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "False\nTrue\n"
    }
   ],
   "source": [
    "print(autograd.is_training())\n",
    "with autograd.record():\n",
    "    print(autograd.is_training())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- autograd also works with python functions and control flows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "a: \n[0.29956347]\n<NDArray 1 @cpu(0)>\n"
    },
    {
     "data": {
      "text/plain": "\n[4096.]\n<NDArray 1 @cpu(0)>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    b = 2*a\n",
    "    while b.norm().asscalar() < 1000:\n",
    "        b = 2*b\n",
    "    if b.sum().asscalar() >0:\n",
    "        c = b\n",
    "    else: \n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "a = nd.random.normal(shape=1)\n",
    "print(\"a:\", a)\n",
    "a.attach_grad()\n",
    "with autograd.record():\n",
    "    d = f(a)\n",
    "d.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- verify: f is piecewise linear in input a. There exists g such that $f(a)=ga$, therefore, $\\frac{\\partial f}{\\partial a}=g$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[4096.]\n<NDArray 1 @cpu(0)>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d/a"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## chain rule\n",
    "- we can break the chain rule manually. for $\\frac{\\partial z}{\\partial x}=\\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial x}$. `y.backward()` will only compute $\\frac{\\partial y}{\\partial x}$. To get $\\frac{\\partial z}{\\partial x}$, we can first compute $\\frac{\\partial z}{\\partial y}$, and then pass it as head gradient to y.backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[0.]\n [2.]\n [4.]\n [6.]]\n<NDArray 4x1 @cpu(0)>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x * 2\n",
    "y.attach_grad()\n",
    "with autograd.record():\n",
    "    z = y*x\n",
    "z.backward()\n",
    "# y.gard = \\partial z / \\partial y\n",
    "y.backward(y.grad)\n",
    "# x.gard = \\partial z / \\partial x\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[0.]\n [2.]\n [4.]\n [6.]]\n<NDArray 4x1 @cpu(0)>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[ 0.]\n [ 4.]\n [ 8.]\n [12.]]\n<NDArray 4x1 @cpu(0)>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.attach_grad()\n",
    "with autograd.record():\n",
    "    y = x * 2\n",
    "    z = y*x\n",
    "z.backward()\n",
    "x.grad"
   ]
  }
 ]
}